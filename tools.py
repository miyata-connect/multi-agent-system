import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_groq import ChatGroq
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from conversation_memory import memory
from cross_context_manager import cross_context

load_dotenv()

# ğŸ”„ ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼šç¾åœ¨ã®ã‚¯ãƒ­ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
_current_cross_context = None

def set_cross_context(cross_context_data):
    """ç¾åœ¨ã®ã‚¯ãƒ­ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š"""
    global _current_cross_context
    _current_cross_context = cross_context_data

# ==========================================
# 1. ç›£æŸ»å½¹ (ChatGPT GPT-5.2)
# ==========================================
def get_auditor_model():
    return ChatOpenAI(
        model="gpt-5.2",
        temperature=0,
        api_key=os.getenv("OPENAI_API_KEY")
    )

@tool
def call_auditor(plan_text: str) -> str:
    """
    ç›£æŸ»å½¹(ChatGPT GPT-5.2): è¨ˆç”»ã®ãƒªã‚¹ã‚¯ã‚’æŒ‡æ‘˜ã—ã¾ã™ã€‚
    è¨ˆç”»æ›¸ã‚„ææ¡ˆã‚’å—ã‘å–ã‚Šã€æŠ€è¡“çš„ãƒªã‚¹ã‚¯ãƒ»ã‚³ã‚¹ãƒˆãƒ»å®Ÿç¾å¯èƒ½æ€§ã‚’å³ã—ãç›£æŸ»ã—ã¾ã™ã€‚
    """
    print(f"\n[System] ğŸ‘®â€â™‚ï¸ ç›£æŸ»å½¹(GPT-5.2) ãƒªã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ä¸­...")
    try:
        # ğŸ¤ ã‚¯ãƒ­ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        context_text = ""
        if _current_cross_context:
            context_text = cross_context.format_for_subordinate(_current_cross_context, 'auditor')
        
        model = get_auditor_model()
        messages = [
            SystemMessage(content=f"""ã‚ãªãŸã¯å†·å¾¹ãªç›£æŸ»å½¹ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨ˆç”»ã«å¯¾ã—ã€æŠ€è¡“çš„ãƒªã‚¹ã‚¯ã€ã‚³ã‚¹ãƒˆè¶…éãƒªã‚¹ã‚¯ã€å®Ÿç¾å¯èƒ½æ€§ã®æ‡¸å¿µç‚¹ã®ã¿ã‚’å³ã—ãæŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚è¤’ã‚ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

{context_text}"""),
            HumanMessage(content=plan_text)
        ]
        response = model.invoke(messages)
        
        # ğŸ§  å›ç­”ã‚’è¨˜æ†¶ã«è¿½åŠ 
        memory.add_session_message('assistant', response.content, 'auditor')
        
        return response.content
    except Exception as e:
        return f"ç›£æŸ»å½¹ã‚¨ãƒ©ãƒ¼: {str(e)}"

# ==========================================
# 2. ã‚³ãƒ¼ãƒ‰å½¹ (Claude Sonnet 4.5)
# ==========================================
def get_coder_model():
    return ChatAnthropic(
        model="claude-sonnet-4-5-20250929",
        temperature=0,
        api_key=os.getenv("ANTHROPIC_API_KEY")
    )

@tool
def call_coder(requirement_text: str) -> str:
    """
    ã‚³ãƒ¼ãƒ‰å½¹(Claude Sonnet 4.5): ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
    è¦ä»¶ã‚’å—ã‘å–ã‚Šã€é«˜å“è³ªã§ãƒã‚°ã®ãªã„å®Œç’§ãªã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¾ã™ã€‚
    """
    print(f"\n[System] ğŸ‘¨â€ğŸ’» ã‚³ãƒ¼ãƒ‰å½¹(Claude Sonnet 4.5) å®Ÿè£…ä¸­...")
    try:
        # ğŸ¤ ã‚¯ãƒ­ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        context_text = ""
        if _current_cross_context:
            context_text = cross_context.format_for_subordinate(_current_cross_context, 'coder')
        
        model = get_coder_model()
        messages = [
            HumanMessage(content=f"""ã‚ãªãŸã¯ä¸–ç•Œæœ€é«˜å³°ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚æ¸¡ã•ã‚ŒãŸè¦ä»¶ã«åŸºã¥ãã€å®Ÿç”¨çš„ã§é«˜å“è³ªãªã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚è§£èª¬ã¯æœ€å°é™ã«ã—ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ¡ã‚¤ãƒ³ã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

{context_text}

ã€è¦ä»¶ã€‘
{requirement_text}""")
        ]
        response = model.invoke(messages)
        
        # ğŸ§  å›ç­”ã‚’è¨˜æ†¶ã«è¿½åŠ 
        memory.add_session_message('assistant', response.content, 'coder')
        
        return response.content
    except Exception as e:
        return f"ã‚³ãƒ¼ãƒ‰å½¹ã‚¨ãƒ©ãƒ¼: {str(e)}"

# ==========================================
# 3. ãƒ‡ãƒ¼ã‚¿å½¹ (Llama 3.3 70B on Groq)
# ==========================================
def get_data_model():
    return ChatGroq(
        model="llama-3.3-70b-versatile",
        temperature=0,
        api_key=os.getenv("GROQ_API_KEY")
    )

@tool
def call_data_processor(text_data: str) -> str:
    """
    ãƒ‡ãƒ¼ã‚¿å½¹(Llama 3.3 70B): ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„ãƒ»å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚
    å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚„è³‡æ–™ã‚’å—ã‘å–ã‚Šã€é«˜é€Ÿã«è¦ç´„ãƒ»æ•´ç†ã—ã¦çµæœã‚’è¿”ã—ã¾ã™ã€‚
    """
    print(f"\n[System] ğŸ¦™ ãƒ‡ãƒ¼ã‚¿å½¹(Llama 3.3 70B) é«˜é€Ÿå‡¦ç†ä¸­...")
    try:
        # ğŸ¤ ã‚¯ãƒ­ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        context_text = ""
        if _current_cross_context:
            context_text = cross_context.format_for_subordinate(_current_cross_context, 'data_processor')
        
        model = get_data_model()
        messages = [
            SystemMessage(content=f"""ã‚ãªãŸã¯å„ªç§€ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¿‚ã§ã™ã€‚æ¸¡ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’è¦ç´„ã—ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

{context_text}"""),
            HumanMessage(content=text_data)
        ]
        response = model.invoke(messages)
        
        # ğŸ§  å›ç­”ã‚’è¨˜æ†¶ã«è¿½åŠ 
        memory.add_session_message('assistant', response.content, 'data_processor')
        
        return response.content
    except Exception as e:
        return f"ãƒ‡ãƒ¼ã‚¿å½¹ã‚¨ãƒ©ãƒ¼: {str(e)}"
